{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_BasicApproach.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsmanoel/pythonLearningAndDeepLearning/blob/master/Tensorflow/Basic/MNIST_BasicApproach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4Y2oKgizotW0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MNIST - DNN Basic Approach (Softmax)\n",
        "\n",
        "A Tensorflow basic exemple of DNN (Densely connected Neural Network) to solve the MNIST dataset without deep layers."
      ]
    },
    {
      "metadata": {
        "id": "90HswbkSeLUZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zdl6AfONo6ZW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P9_ZGuoTpNDt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k1oIFV9xr42-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Placeholders:**"
      ]
    },
    {
      "metadata": {
        "id": "K-eumw_Kr-Ob",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "m = 784 #number of inputs\n",
        "n = 10  #number of outputs\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, m])\n",
        "y_true = tf.placeholder(tf.float32, [None, n])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JvmwtQwAsP7G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Variables:**"
      ]
    },
    {
      "metadata": {
        "id": "TXkkDjK0sWhB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w = tf.Variable(tf.zeros([m, n]))\n",
        "b = tf.Variable(tf.zeros([n]))\n",
        "\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gtt3ocC0scIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Graph:**\n",
        "\n",
        "![alt text](https://www.researchgate.net/profile/Enio_Pereira/publication/229036664/figure/fig2/AS:300716544544769@1448707819175/Artificial-Neuron-models-and-its-parts-Source-Adapted-from-Haykin-1994.png)\n",
        "\n",
        "Softmax Activation Function: \n",
        "$$\\varphi(\\cdot)=\\varphi(v_j)=\\frac{e^{v_j}}{\\sum_{i=0}^{n} e^{v_i}}$$\n",
        "This activation function is used because your range will 0 to 1, and the sum of all the probabilities will be equal to one. This characteristic is necessary to solve multiclass problems.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "C8ecFRZcsbKQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = tf.matmul(x, w) + b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xwVEcJH6sjiN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Loss Function**\n",
        "\n",
        "**Surprisal:** Degree of surprise for a result obtained.\n",
        "$$s = log\\left(\\frac{1}{y_j}\\right)$$\n",
        "\n",
        "**Entropy:** Weighted average of surprisals obtained. Since we know the probability of each outcome, this probability can be considered as the weight\n",
        "$$e = \\sum_{j=0}^{n} y_j \\times log\\left(\\frac{1}{y_j}\\right)$$\n",
        "\n",
        "**Cross Entropy:** Weighted average of surprisals obtained. Now the weighting is reached by the real probability $y\\_true_j$ (considering that the probability obtained, $y_j$, is not necessarily the correct one).\n",
        "$$c = \\sum_{j=0}^{n} y\\_true_j \\times log\\left(\\frac{1}{y_j}\\right)$$\n",
        "When the estimated probability distribution moves away from truel/desired probability distribution, cross entropy increases and vice-versa. That's why this formula is also called cross Cross Entropy Loss. The training process will attempt to minimize the Cross Entropy Loss."
      ]
    },
    {
      "metadata": {
        "id": "BSOmG8YIskAA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, \n",
        "                                                                          logits=y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gSh6rjSnssXq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Optimizer:**\n",
        "\n",
        "[About optomizers](http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms)\n",
        "\n",
        "[GradientDescentOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer)"
      ]
    },
    {
      "metadata": {
        "id": "zXPSVg1dsrXo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\n",
        "train = optimizer.minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DHy7y28csyTU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Session:**"
      ]
    },
    {
      "metadata": {
        "id": "ztjy-IWRsy6m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7e3c0a35-9c67-4a1a-8c89-8fe6efbd74b1"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  \n",
        "  sess.run(init)\n",
        "  \n",
        "  for step in range(1000):\n",
        "    \n",
        "    batch_x, batch_y = mnist.train.next_batch(100)\n",
        "    \n",
        "    sess.run(train, feed_dict={x:batch_x, y_true:batch_y})\n",
        "  \n",
        "  #-----------------------------------------------------------------------------\n",
        "  #Evaluation:\n",
        "  correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_true, 1))\n",
        "  \n",
        "  #[1.0, 0.0, 1.0, ...] <--(cast to float) [True, False, True, ...]\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  \n",
        "  #-----------------------------------------------------------------------------\n",
        "  #Test:\n",
        "  print(sess.run(accuracy, feed_dict={x:mnist.test.images, y_true:mnist.test.labels}))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9166\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}